\section{Hidden Markov Tree}
\subsection{General Introduction}
Segmentation of an image into unknown number of distinct and in some way homogeneous regions has been a fundamental issue in low level image analysis. There are many direct application of the algorithm, for example, segmentation of \gls{cti}, \gls{pet}, segmentation of \gls{nmr} images, and other applications in radar, satellite and agriculture. Many types of approaches has been in practice and in this paper a probabilistic method of segmentation is employed, \gls{hmm} based.

\subsection{Overview of Hidden Markov Model}
The \gls{hmt} was introduced by Crouse, Nowak and Baraniuk (1998). The context of their work was the modeling of statistical dependencies between wavelet coefficients in signal processing, for which variables are organized in a natural tree structure. Applications of such a model are: image segmentation, signal class cation, denoising and image document categorization \cite{durand2001statistical}.

A \gls{hmm} is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. In a hidden Markov model, the state is not directly visible, but the output, dependent on the state, is visible. Each state has a probability distribution over the possible output tokens. Therefore, the sequence of tokens generated by an \gls{hmm} gives some information about the sequence of states. The adjective 'hidden' refers to the state sequence through which the model passes, not to the parameters of the model; the model is still referred to as a 'hidden' Markov model even if these parameters are known exactly \cite{jezicagent}. 
% from https://en.wikipedia.org/wiki/Hidden_Markov_model

The unifying ideas in using Markov Random Fields for vision are the following:
\begin{itemize}
	%\setlength\itemsep{1em}
	\item Images are dissected into an assembly of nodes that may correspond to pixels.
	\item Hidden variables associated with the nodes are introduced into a model designed to “explain” the values (colors) of all the pixels.
	\item  A joint probabilistic model is built over the pixel values and the hidden variables
	\item The direct statistical dependencies between hidden  variables are expressed by explicitly grouping hidden variables; these groups are often pairs depicted as edges in a graph.
\end{itemize}
These properties are illustrated in Figure \ref{fig:connectivity} below:
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{fig/Connectivity.PNG}
	\caption{Graphs for Markov models in vision. (a) Simple 4-connected grid of image pixels. (b) Grids with greater connectivity
		here with the 8-connected pixel grid.}
	\label{fig:connectivity}
\end{figure}

The motivation for constructing such a graph is to connect the hidden variables associated with the nodes. For example, for the task of segmenting an image into foreground and background, each node i (pixel) has an associated random variable $X_i$ that may take the value 0 or 1, corresponding to foreground or background, respectively may take the value 0 or 1, corresponding to foreground or background, respectively. In order to represent the tendency of matter to be coherent, neighboring sites are likely to have the same label. So where $(i, j) \in E$, some kind of probabilistic bias needs to be associated with the edge $(i,j)$ such that $X_i$ and $X_J$ tend to have the same label—both 0 or both 1. In fact, any pixels that are nearby, not merely adjacent, are likely to have the same label\cite{jezicagent}. 



123